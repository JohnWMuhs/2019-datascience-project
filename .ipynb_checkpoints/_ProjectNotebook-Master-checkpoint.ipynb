{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron 2.0: Predicting California's Energy Consumption\n",
    "\n",
    "## Team Members:\n",
    "\n",
    "**Names:** John W. Muhs, Corbett Carell\n",
    "<br>**Emails:** <u0761102,u0502104>@utah.edu</br>\n",
    "<br>**Github Repository:** [JohnWMuhs/2019-datascience-project](https://github.com/JohnWMuhs/2019-datascience-project \"2019 Data Science Project Github Repo\")</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Objectives\n",
    "\n",
    "In this project, we present a power consumption prediction method for the [California ISO](http://www.caiso.com/Pages/default.aspx). CAISO is California's largest Balancing Authority and Market Independent System Operator. CAISO holds a number of responsibilities and functions to ensure a competitive, cost-effective, reliabile, and environmentally friendly energy market. For example, CAISO oversees market transactions between power industry stakeholders in California such as Generation Power Plants, Transmission Owners, Renewables Integrators and Customers.  As such, CAISO closely monitors large-scale generation and consumption trends for the State of California. \n",
    "\n",
    "By utilizing hourly CAISO load data from the Energy Information Agency (EIA), weather data from the Darksky API. CAISO As of Milestone 1, only these data sets are used, and correlation between weather variables and load has been found, but other data sets may be included to further improve the model. \n",
    "\n",
    "<img src=\"images/CaliforniaISO_PoweringLivesEconomy.jpg\">\n",
    "\n",
    "<img src=\"images/CaliforniaISO_OpenTransparentMrkt.jpg\">\n",
    "\n",
    "<img src=\"images/CaliforniaISO_GreeningGridFutureGen.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "<br>**Project Proposal (Submitted Mar. 1st):** The project was introduced via a project proposal available on Google Docs [here](https://docs.google.com/document/d/1i6FB5gmumkx5CnaKLHzKJk8nae0PirpgJDDWm3NkkZ4/edit?usp=sharing \"Project Proposal\").</br>\n",
    "\n",
    "Notes from the peer review session are included in the project proposal on the last page. We have also included the project proposal in a pdf included in the Github repository. \n",
    "\n",
    "**Things to add from First Milestone:**\n",
    "1. Weekday vs. Weekend (Done via Pandas)\n",
    "2. Number of Customer Accounts ([EIA](https://www.eia.gov/opendata/qb.php?category=1718389))\n",
    "3. Percent Urban Population of California ([BEA](https://apps.bea.gov/API/signup/index.cfm))\n",
    "4. Gross State Product (GSP) and/or GDP of USA ([BEA](https://apps.bea.gov/API/signup/index.cfm))\n",
    "5. Split the weather data into categories\n",
    "6. Try PCA\n",
    "7. Try a Linear Regression, kNN Regression, and simple perceptron. \n",
    "8. Use a mean error to measure accuracy of regression\n",
    "9. Could use the mean error from the best model to model accuracy on the other models. \n",
    "\n",
    "\n",
    "\n",
    "## Data Sources\n",
    "<br>**Energy Data:** [U.S. Electric System Operating Data](https://www.eia.gov/opendata/qb.php?category=2123635 \"EIA API: Electric System Operating Data\")</br>\n",
    "<br>**Weather Data:** [Darksky API](https://darksky.net/dev \"Darksky API\")</br>\n",
    "\n",
    "## Project Timeline\n",
    "\n",
    "\\<img src=\"images/ProjectFlowChart_Proposal.png\">\n",
    "\n",
    "## Feedback from Peer Review Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Data Import and Formatting\n",
    "\n",
    "This section includes the code required to import power consumption from the CAISO Balancing Authority (BA) and the entire California region. The California region contains a number of BAs (the largest of which by far is CAISO), however, we decided to import both BA and region data for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests \n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below allows a user to quickly copy/paste a link from the EIA API website, and provide a title for it in the urls dictionary. The code takes that url and creates a unified dataframe. Therefore, it is incredibly easy to add additional EIA data in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EIA API Key \n",
    "EIA_api_key = '53e6a63887dc05efe150165fa890f8da'\n",
    "\n",
    "# Create Dictionary of urls from which we want to pull the EIA API\n",
    "urls = {'CAISO_HourlyLoad':'http://api.eia.gov/series/?api_key='+EIA_api_key+'&series_id=EBA.CISO-ALL.D.H',\n",
    "        'California_HourlyLoad':'http://api.eia.gov/series/?api_key='+EIA_api_key+'&series_id=EBA.CAL-ALL.D.H'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Setup dummy variable to ensure that a datetime is only generated once\n",
    "i = 0\n",
    "\n",
    "for key in urls:\n",
    "    \n",
    "    # API pull\n",
    "    EIAData = requests.get(urls[key])\n",
    "    \n",
    "    # Decode from utf-8\n",
    "    EIAData = EIAData.content.decode(\"utf-8\")\n",
    "    \n",
    "    # Load API pull as a dict\n",
    "    EIAdict = json.loads(EIAData)\n",
    "    \n",
    "    # Access the data in EIAdict\n",
    "    dfEIA = pd.DataFrame.from_dict(EIAdict['series'])\n",
    "    dfEIA = dfEIA['data'][0]\n",
    "    \n",
    "    # Convert clean data to a dataframe\n",
    "    dfEIA = pd.DataFrame(dfEIA)\n",
    "    #print(dfEIA[0])\n",
    "    \n",
    "    # Find the datetime\n",
    "    while i != 1:\n",
    "        df['DateTime'] = pd.to_datetime(dfEIA[0],format='%Y%m%dT%H', errors='ignore')\n",
    "        df['DateTime'] = df['DateTime'].dt.tz_convert('US/Pacific')\n",
    "        i += 1\n",
    "    \n",
    "    # Create a new column for each type of data (url) pulled from API\n",
    "    df[str(key)] = dfEIA[1]\n",
    "    \n",
    "# Set DateTime as index of the dataframe (tz-aware)\n",
    "df.index = df['DateTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to complete the project in a daily time resolution. This was done mainly to reduce the dataset to a reasonable size. If we kept the dataset at an hourly time resolution, we would have over 32,000 data points! However converting the data down to a daily time resolution provided a very nicely-sized 1362 data points from which to analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample hourly data to daily data (summed energy consumption which has units of MWh)\n",
    "df = df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that determines whether or not a date is a weekday or weekend day.\n",
    "df['isWeekend'] = np.ones(df.shape[0]).astype(int)\n",
    "df['isWeekend'] = df['isWeekend'].where((df.index.weekday == 5) | (df.index.weekday == 6),0).astype(int)\n",
    "\n",
    "#Add index of Month and Day of Year\n",
    "df['Month'] = df.index.month\n",
    "df['DayofYear'] = df.index.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Customer Accounts (pulled from EIA API in file \"EIA_API-MonthlyCustomerCount.ipynb\")\n",
    "dfCustomerAccts = pd.read_csv('CustomerAccts.csv')\n",
    "\n",
    "# Parse DateTime from string to pandas datetime\n",
    "dfCustomerAccts['DateTime'] = pd.to_datetime(dfCustomerAccts['DateTime'],format='%Y-%m-%d', errors='ignore')\n",
    "\n",
    "# Set datetime as index\n",
    "dfCustomerAccts.index = dfCustomerAccts['DateTime']\n",
    "\n",
    "# Delete extraneous column\n",
    "dfCustomerAccts = dfCustomerAccts.drop('DateTime',axis=1)\n",
    "\n",
    "# Localize Timezone to Pacific\n",
    "dfCustomerAccts = dfCustomerAccts.tz_localize('US/Pacific', level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,dfCustomerAccts, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataframe looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following code to export this dataframe to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('energydata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data (DarkSky API) Import and Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather data was pulled in using the DarkSky API. The Darksky API requires inputs of latitude, longitude, and datetime to pull corresponding weather data. Darksky API does not seem to allow pulls over a date range, so for this reason, the code to pull over the entire date range required over 6,000 pulls from the API. So as not to do this every time we run the code, a separate code has been developed and included in the repository that demonstrates how DarkSky data was accessed. The code to pull the weather data can be found in \"darksky-api.ipynb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data was pulled from July 1st, 2015 (constraint from EIA hourly data) to December 31st, 2018. All attributes from the DarkSky API pull have been included in the csv, so all filtering and processing will be included in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull weather data from static csv\n",
    "dfWeather = pd.read_csv('darksky_data.csv')\n",
    "\n",
    "#Set dummy variable\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid renaming multiple times and pulling an error (we could include a try except here.)\n",
    "while i != 1:\n",
    "    #Take the date time (originally included as a string), and format it into a pd datetime\n",
    "    dfWeather['DateTime'] = pd.to_datetime(dfWeather['time'],format='%m-%d-%Y', errors='ignore')\n",
    "    \n",
    "    # Set Datetime as index of the dataframe\n",
    "    dfWeather.index = dfWeather['DateTime']\n",
    "    \n",
    "    # Drop extraneous columns\n",
    "    dfWeather = dfWeather.drop(['time','DateTime'],axis=1)\n",
    "    \n",
    "    # Set TZ to US-Pacific (both dataframes are tz-aware)\n",
    "    dfWeather = dfWeather.tz_localize('US/Pacific', level=0)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Data Merge and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge EIA and Weather dataframes. Inner only keeps the dates from the dfWeather dataframe. \n",
    "dfX = pd.merge(df,dfWeather, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "# Display head of merged dataframe\n",
    "dfX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = pd.read_csv('calpop.csv')\n",
    "df_di = pd.Series(pd.date_range(start='7/1/2015', end = '12/31/2018', tz = 'US/Pacific'))\n",
    "df_pop = pd.concat([df_di, df_pop], axis = 1)\n",
    "df_pop['Datetime'] = df_pop[0]\n",
    "df_pop.index = df_pop['Datetime']\n",
    "df_pop.drop(['Date', 'Datetime', 0],axis = 1, inplace = True)\n",
    "\n",
    "df_pop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdp = pd.read_csv('bea_gdp.csv')\n",
    "df_gdp = pd.concat([df_di, df_gdp], axis = 1)\n",
    "df_gdp['Datetime'] = df_gdp[0]\n",
    "df_gdp.index = df_gdp['Datetime']\n",
    "df_gdp.drop(['Date', 'Datetime', 0],axis = 1, inplace = True)\n",
    "\n",
    "df_gdp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = pd.concat([dfX, df_gdp, df_pop], axis=1)\n",
    "dfX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping na values and dropping California houylr_load since we will be using CASIO_hourlyload data as our depenedent variable for models\n",
    "dfX = dfX.dropna()\n",
    "dfX = dfX.drop(['California_HourlyLoad'], axis = 1)\n",
    "dfX = dfX[dfX['CAISO_HourlyLoad'] > 400000]\n",
    "dfX.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0,ax1,ax2) = plt.subplots(nrows=3, figsize=(18, 10))\n",
    "#start_date = [\"2016-01-01\",\"2017-01-01\",\"2018-01-01\"]\n",
    "#end_date = [\"2016-12-31\",\"2017-12-31\",\"2018-12-31\"]\n",
    " \n",
    "#ax0.set_title('CAISO Consumption 2016 - 2018')\n",
    "ax0.plot(dfX['CAISO_HourlyLoad'])\n",
    "ax0.set_xlabel('Date')\n",
    "ax0.set_ylabel('Consumption (MWh)')\n",
    "ax1.plot(dfX['temperatureMin'])\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Min Daily Temperature (Celsius)')\n",
    "ax2.plot(dfX['dewPoint'])\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Dew Point (Celsius)')\n",
    "\n",
    "#df2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Data Exploration through Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use a random forest to help us understand which variable are most important to determining load. This will allow us to create a new data frame with fewer variables in order to eliminate uninformative variables from our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_sample2, testing_sample2 = train_test_split(dfX, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Training data set size = \", len(training_sample2))\n",
    "print(\"Testing data set size = \", len(testing_sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(training_sample2['CAISO_HourlyLoad'])\n",
    "train_features = training_sample2.drop('CAISO_HourlyLoad', axis=1)\n",
    "\n",
    "training_sample2 = train_features\n",
    "train_feature_list = list(train_features.columns)\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 400, random_state = 42, min_impurity_decrease = 0.01)\n",
    "\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num of features when fit is perform')\n",
    "print(rf.n_features_)\n",
    "print('--------------------')\n",
    "print('Num of outputs when fit is performed')\n",
    "print(rf.n_outputs_)\n",
    "print('--------------------')\n",
    "print('Feature Importance')\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Here we are determining the accuracy of our random forest\n",
    "test_labels = np.array(testing_sample2['CAISO_HourlyLoad'])\n",
    "test_features = testing_sample2.drop('CAISO_HourlyLoad', axis=1)\n",
    "\n",
    "testing_sample2 = test_features\n",
    "test_feature_list = list(test_features.columns)\n",
    "\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "accuracy = cross_validate(rf, test_features, test_labels, cv=10)['test_score']\n",
    "print('The accuracy is: ',sum(accuracy)/len(accuracy)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, train_feature_list, rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the importance of the variables to create a new dataframe of only the 10 most important variables in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(rf.feature_importances_)\n",
    "importances\n",
    "names = pd.Series(train_feature_list)\n",
    "names\n",
    "import_df = pd.concat([importances, names], axis=1)\n",
    "import_df\n",
    "skim_df = import_df.sort_values(by=[0] , ascending = False) \n",
    "skim_df.reset_index(inplace = True)\n",
    "del skim_df[\"index\"]\n",
    "thin_df = skim_df.loc[0:9]\n",
    "print(thin_df)\n",
    "column_list = list(skim_df.iloc[0:10,1])\n",
    "column_list.append('CAISO_HourlyLoad')\n",
    "print(column_list)\n",
    "df_slim = dfX[column_list]\n",
    "\n",
    "df_slim.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using 2 scatter plot matrices to get a visual of the interaction between our different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim_1 = dfX[['CAISO_HourlyLoad', \n",
    "                    'apparentTemperatureMin', \n",
    "                    'temperatureMin', \n",
    "                    'apparentTemperatureLow', \n",
    "                    'isWeekend', \n",
    "                    'temperatureLow']]\n",
    "\n",
    "pd.plotting.scatter_matrix(df_slim_1, figsize=(30, 30), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim_2 = dfX[['CAISO_HourlyLoad',\n",
    "                 'apparentTemperatureHigh', \n",
    "                 'apparentTemperatureMax', \n",
    "                 'DayofYear', \n",
    "                 'temperatureHigh', \n",
    "                 'temperatureMax']]\n",
    "\n",
    "pd.plotting.scatter_matrix(df_slim_2, figsize=(30, 30), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "ols1 = sm.ols(formula=\"CAISO_HourlyLoad ~ temperatureMin + dewPoint\", \n",
    "              data=dfX).fit()\n",
    "\n",
    "ols1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** I decided to use the 2 most important variables for our first regression model. We get good results for our first model. Our p-values are very low and the F-stat is very high which are both good signs that the model is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2 = sm.ols(formula=\"CAISO_HourlyLoad ~ temperatureMin\", data=dfX).fit()\n",
    "ols2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** By eliminating the dewPoint from the regression we see a huge increase in the F-stat for the model,  however, the R-squared dropped. These mean that the model is more statistically significant than the previous model but doesn't fit the data as well as the first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols3 = sm.ols(formula=\"CAISO_HourlyLoad ~ I(temperatureMin ** 5)\", data=dfX).fit()\n",
    "ols3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols4 = sm.ols(formula=\"CAISO_HourlyLoad ~ I(temperatureMin ** 5) + dewPoint + isWeekend\", data=dfX).fit()\n",
    "ols4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols4 = sm.ols(formula=\"CAISO_HourlyLoad ~ I(temperatureMin ** 7)\", data=dfX).fit()\n",
    "ols4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_test = sm.ols(formula=\"CAISO_HourlyLoad ~ apparentTemperatureMin + I(temperatureMin ** 3) + dewPoint + isWeekend\", \n",
    "              data=dfX).fit()\n",
    "\n",
    "ols_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "x = (dfX[['apparentTemperatureMin', \n",
    "          'temperatureMin', \n",
    "          'apparentTemperatureLow', \n",
    "          'isWeekend', \n",
    "          'temperatureLow']].values)\n",
    "\n",
    "y = dfX['CAISO_HourlyLoad'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "     \n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "dif_lr = y_test_lr - y_pred_lr\n",
    "# print(dif_lr)\n",
    "dif_lr_s = pd.Series(dif_lr)\n",
    "print(dif_lr_s.describe())\n",
    "ypred_mean_lr = np.mean(dif_lr)\n",
    "load_mean_lr = np.mean(y_test_lr)\n",
    "print('-------------------------------------')\n",
    "print(ypred_mean_lr/load_mean_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** After looking at the scatter plot matrix I noticed that CAISO_HourlyLoad and temperatureMin could have an exponential relationship so I decided to test a few linear models with different exponents. By increasing the exponential power of the temperatureMin variable in the model we are able to increase both the F-stat and R-squared. One thing to note is when we increase the power to 7 is when we start to see the F-stat and R-squared begin to decrease instead of increase. A temperatureMin with a power of 6 is the best linear model I found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps:** We need to use cross-validation to verify that our model is accurate and that we have not overfitted the model to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale \n",
    "\n",
    "X = dfX[['apparentTemperatureMin',\n",
    "         'temperatureMin',\n",
    "         'dewPoint',\n",
    "         'isWeekend',\n",
    "         'apparentTemperatureMax',\n",
    "         'apparentTemperatureLow',\n",
    "         'CA_CustomerCount',\n",
    "         'apparentTemperatureHigh',\n",
    "         'moonPhase',\n",
    "         'temperatureLow']]\n",
    "\n",
    "X_scale = scale(X)\n",
    "\n",
    "pca_model = PCA()\n",
    "energy_pca = pca_model.fit_transform(X_scale)\n",
    "\n",
    "pca_df = pd.DataFrame(energy_pca, columns=['PC1', 'PC2',\n",
    "                                           'PC3', 'PC4', 'PC5','PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "\n",
    "# Variance ratio of the four principal components\n",
    "var_ratio = pca_model.explained_variance_ratio_\n",
    "print(var_ratio)\n",
    "print('-------------------------------------')\n",
    "\n",
    "plt.plot(range(len(pca_df.columns)), var_ratio, '-o')\n",
    "\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.xlim(0.75,4.25)\n",
    "plt.ylim(0,1.05)\n",
    "plt.xticks(range(len(pca_df.columns)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_ = 'OrRd'\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "y = dfX.CAISO_HourlyLoad\n",
    "\n",
    "plt.scatter(energy_pca[:, 0], energy_pca[:, 1], c=y, cmap=cmap_, s=30)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.index = dfX.index\n",
    "dfPCA = pd.concat([dfX, pca_df], axis = 1)\n",
    "# dfPCA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_test = sm.ols(formula=\"CAISO_HourlyLoad ~ PC1 + PC2 + PC3 + PC4 + PC5\", \n",
    "              data=dfPCA).fit()\n",
    "\n",
    "ols_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "x = (dfX[['temperatureMin', 'dewPoint']].values)\n",
    "y = (dfX['CAISO_HourlyLoad'].values).reshape(-1, 1)\n",
    "\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "     \n",
    "lr.fit(X_train_lr, y_train_lr)\n",
    "y_pred_lr = lr.predict(X_test_lr)\n",
    "\n",
    "dif_lr = y_test_lr - y_pred_lr\n",
    "# print(dif_lr)\n",
    "dif_lr_s = pd.Series(dif_lr)\n",
    "print(dif_lr_s.describe())\n",
    "ypred_mean_lr = np.mean(dif_lr)\n",
    "load_mean_lr = np.mean(y_test_lr)\n",
    "print('-------------------------------------')\n",
    "print(ypred_mean_lr/load_mean_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = dfX[['apparentTemperatureMin',\n",
    "         'temperatureMin',\n",
    "         'dewPoint',\n",
    "         'isWeekend',\n",
    "         'apparentTemperatureMax',\n",
    "         'apparentTemperatureLow',\n",
    "         'CA_CustomerCount',\n",
    "         'apparentTemperatureHigh',\n",
    "         'moonPhase',\n",
    "         'temperatureLow']]\n",
    "\n",
    "y = dfX['CAISO_HourlyLoad']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=.3, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP regression with Scikit-Learn\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(100,100,100,100,100), \n",
    "                       verbose=0, \n",
    "                       random_state=2, \n",
    "                       solver='adam')\n",
    "print(mlp_reg.get_params())\n",
    "\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "y_pred = mlp_reg.predict(X_test)\n",
    "\n",
    "# print(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(mlp_reg.score(X_test,y_test)) # score = 1 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = y_test - y_pred\n",
    "dif_s = pd.Series(dif)\n",
    "print(dif_s.describe())\n",
    "ypred_mean = abs(np.mean(dif))\n",
    "load_mean = np.mean(y_test)\n",
    "print('-------------------------------------')\n",
    "print(ypred_mean/load_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP regression with Scikit-Learn\n",
    "X = pca_df[['PC1',\n",
    "           'PC2',\n",
    "           'PC3',\n",
    "           'PC4',\n",
    "           'PC5']]\n",
    "\n",
    "y = dfX['CAISO_HourlyLoad']\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X, y, test_size=.3, random_state = 1)\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(100,100,100,100,100), \n",
    "                       verbose=0, \n",
    "                       random_state=2, \n",
    "                       solver='adam')\n",
    "\n",
    "print(mlp_reg.get_params())\n",
    "\n",
    "mlp_reg.fit(X_train_pca, y_train_pca)\n",
    "y_pred_pca = mlp_reg.predict(X_test_pca)\n",
    "\n",
    "# print(mean_squared_error(y_test_pca, y_pred_p))\n",
    "\n",
    "print(mlp_reg.score(X_test_pca,y_test_pca)) # score = 1 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif_pca = y_test_pca - y_pred_pca\n",
    "dif_pca_s = pd.Series(dif_pca)\n",
    "print(dif_pca_s.describe())\n",
    "ypred_mean_pca = abs(np.mean(dif_pca))\n",
    "load_mean_pca = np.mean(y_test_pca)\n",
    "print('-------------------------------------')\n",
    "print(ypred_mean_pca/load_mean_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
